#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fix_english_leakage.py â€” æ£€æµ‹å¹¶ä¿®å¤ç¿»è¯‘ä¸­æ®‹ç•™çš„è‹±æ–‡å•è¯

åŠŸèƒ½:
1. æ£€æµ‹è¯‘æ–‡ä¸­çš„è‹±æ–‡å•è¯ï¼ˆæ’é™¤ä¸“æœ‰åè¯ã€å˜é‡ï¼‰
2. è‡ªåŠ¨é‡æ–°ç¿»è¯‘æœ‰é—®é¢˜çš„å¥å­
3. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
4. æ”¯æŒæ‰¹é‡å¤„ç†

ç”¨æ³•ç¤ºä¾‹:
  # æ£€æµ‹å¹¶æŠ¥å‘Šé—®é¢˜
  python tools/fix_english_leakage.py outputs/test_basement/llm_results/batch_0001.jsonl --check-only

  # è‡ªåŠ¨ä¿®å¤ï¼ˆé‡æ–°ç¿»è¯‘ï¼‰
  python tools/fix_english_leakage.py outputs/test_basement/llm_results/batch_0001.jsonl --fix --model qwen3:8b

  # æ‰¹é‡å¤„ç†ç›®å½•
  python tools/fix_english_leakage.py outputs/test_basement/llm_results --fix --model qwen3:8b
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Optional
from urllib import request as urlreq

# å¸¸è§çš„Ren'Pyå˜é‡å’Œä¸“æœ‰åè¯æ¨¡å¼
ALLOWED_ENGLISH = {
    # Ren'Py å˜é‡
    'pov', 'mom', 'ls', 'mc', 'npc', 'ui',
    # å¸¸è§ä¸“æœ‰åè¯
    'ok', 'yes', 'no', 'save', 'load', 'menu',
    # å•å­—æ¯
    'a', 'i',
}

# æ£€æµ‹è‹±æ–‡å•è¯çš„æ­£åˆ™ï¼ˆæ’é™¤å ä½ç¬¦ï¼‰
ENGLISH_WORD_PATTERN = re.compile(
    r'\b[a-zA-Z]{2,}(?:\'[a-z]+)?\b'  # è‡³å°‘2ä¸ªå­—æ¯çš„è‹±æ–‡å•è¯
)

# å ä½ç¬¦æ¨¡å¼
PLACEHOLDER_PATTERN = re.compile(
    r'\[[A-Za-z_][A-Za-z0-9_]*\]'  # [name], [pov]
    r'|\{[A-Za-z_][^}]*\}'  # {i}, {color=#fff}
)


def strip_placeholders(text: str) -> str:
    """ç§»é™¤å ä½ç¬¦"""
    return PLACEHOLDER_PATTERN.sub('', text)


def detect_english_words(text: str) -> list[str]:
    """
    æ£€æµ‹è¯‘æ–‡ä¸­çš„è‹±æ–‡å•è¯
    
    Returns:
        æ®‹ç•™çš„è‹±æ–‡å•è¯åˆ—è¡¨
    """
    # ç§»é™¤å ä½ç¬¦åæ£€æµ‹
    clean_text = strip_placeholders(text)
    
    # æŸ¥æ‰¾æ‰€æœ‰è‹±æ–‡å•è¯
    words = ENGLISH_WORD_PATTERN.findall(clean_text)
    
    # è¿‡æ»¤å…è®¸çš„è¯
    leaked = [
        word for word in words
        if word.lower() not in ALLOWED_ENGLISH
    ]
    
    return leaked


def analyze_jsonl(jsonl_path: Path) -> tuple[list[dict], int, int]:
    """
    åˆ†æ JSONL æ–‡ä»¶
    
    Returns:
        (æœ‰é—®é¢˜çš„æ¡ç›®, æ€»æ•°, é—®é¢˜æ•°)
    """
    problematic: list[dict] = []
    total = 0
    
    with jsonl_path.open('r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            
            total += 1
            try:
                obj = json.loads(line)
                item_id = obj.get('id', '')
                zh = obj.get('zh', '')
                
                if not zh:
                    continue
                
                # æ£€æµ‹è‹±æ–‡æ®‹ç•™
                leaked = detect_english_words(zh)
                if leaked:
                    problematic.append({
                        'id': item_id,
                        'zh': zh,
                        'leaked_words': leaked,
                    })
            
            except (ValueError, json.JSONDecodeError):
                continue
    
    return problematic, total, len(problematic)


def build_fix_prompt() -> str:
    """æ„å»ºä¿®å¤ç¿»è¯‘çš„ç³»ç»Ÿæç¤ºè¯"""
    return (
        "ä½ æ˜¯ä¸“ä¸šç¿»è¯‘è´¨é‡ä¿®æ­£å‘˜ã€‚ç”¨æˆ·ä¼šç»™ä½ ä¸€æ®µä¸­è‹±æ··åˆçš„ç¿»è¯‘ï¼Œä½ çš„ä»»åŠ¡æ˜¯å°†å…¶ä¸­çš„è‹±æ–‡å•è¯æ›¿æ¢ä¸ºå¯¹åº”çš„ä¸­æ–‡ã€‚\n\n"
        
        "ã€ä¿®æ­£è§„åˆ™ã€‘\n"
        "- ä¿æŒåŸæœ‰çš„ä¸­æ–‡ä¸å˜\n"
        "- åªæ›¿æ¢è‹±æ–‡å•è¯ä¸ºæ°å½“çš„ä¸­æ–‡\n"
        "- ä¿æŒå ä½ç¬¦ï¼ˆ[name], {i} ç­‰ï¼‰ä¸å˜\n"
        "- ä¿æŒå¥å­ç»“æ„å’Œè¯­æ°”\n"
        "- ç¡®ä¿ä¿®æ­£åçš„è¯‘æ–‡å®Œå…¨æ˜¯ä¸­æ–‡\n\n"
        
        "ã€ç¤ºä¾‹ã€‘\n"
        "è¾“å…¥: ä½  also ä¹Ÿå–œæ¬¢è¿™ä¸ª\n"
        "è¾“å‡º: ä½ ä¹Ÿå–œæ¬¢è¿™ä¸ª\n\n"
        
        "è¾“å…¥: äº«å—ä½ çš„ pleasure\n"
        "è¾“å‡º: äº«å—ä½ çš„å¿«æ„Ÿ\n\n"
        
        "è¾“å…¥: ä¸€ä¸ªdirtyçš„å°ç§˜å¯†\n"
        "è¾“å‡º: ä¸€ä¸ªè‚®è„çš„å°ç§˜å¯†\n\n"
        
        "ã€è¾“å‡ºè¦æ±‚ã€‘\n"
        "- åªè¾“å‡ºä¿®æ­£åçš„ä¸­æ–‡è¯‘æ–‡\n"
        "- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹"
    )


def fix_translation(
    text: str,
    host: str = "http://127.0.0.1:11434",
    model: str = "qwen3:8b",
    timeout: float = 30.0
) -> Optional[str]:
    """
    ä½¿ç”¨ Ollama ä¿®å¤ç¿»è¯‘
    
    Args:
        text: æœ‰é—®é¢˜çš„è¯‘æ–‡
        host: Ollama åœ°å€
        model: æ¨¡å‹åç§°
        timeout: è¶…æ—¶æ—¶é—´
    
    Returns:
        ä¿®æ­£åçš„è¯‘æ–‡ï¼Œå¤±è´¥è¿”å› None
    """
    url = host.rstrip("/") + "/api/chat"
    
    system_prompt = build_fix_prompt()
    user_prompt = f"è¯·ä¿®æ­£ä»¥ä¸‹ç¿»è¯‘ä¸­çš„è‹±æ–‡å•è¯ï¼Œå°†å®ƒä»¬æ›¿æ¢ä¸ºæ°å½“çš„ä¸­æ–‡ï¼š\n\n{text}"
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": False,
        "options": {"temperature": 0.1},  # ä½æ¸©åº¦ï¼Œä¿å®ˆè¾“å‡º
    }
    
    try:
        data = json.dumps(payload).encode("utf-8")
        req = urlreq.Request(url, data=data, headers={"Content-Type": "application/json"})
        
        with urlreq.urlopen(req, timeout=timeout) as resp:
            result = json.loads(resp.read().decode("utf-8", errors="ignore"))
            fixed = (result.get("message") or {}).get("content") or ""
            return fixed.strip()
    
    except Exception as e:
        print(f"  âœ— ä¿®å¤å¤±è´¥: {e}")
        return None


def generate_report(problematic: list[dict], total: int, output_path: Path):
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    with output_path.open('w', encoding='utf-8') as f:
        f.write("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        f.write("ç¿»è¯‘è´¨é‡æ£€æµ‹æŠ¥å‘Š\n")
        f.write("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n")
        
        f.write(f"æ€»ç¿»è¯‘æ•°: {total}\n")
        f.write(f"é—®é¢˜æ•°: {len(problematic)}\n")
        f.write(f"é—®é¢˜ç‡: {100 * len(problematic) / total:.2f}%\n\n")
        
        f.write("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        f.write("é—®é¢˜è¯¦æƒ…\n")
        f.write("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n")
        
        # æŒ‰æ®‹ç•™è¯æ±‡åˆ†ç»„
        by_word: dict[str, list[dict]] = {}
        for item in problematic:
            for word in item['leaked_words']:
                word_lower = word.lower()
                if word_lower not in by_word:
                    by_word[word_lower] = []
                by_word[word_lower].append(item)
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(by_word.items(), key=lambda x: len(x[1]), reverse=True)
        
        for word, items in sorted_words:
            f.write(f"ã€{word}ã€‘ å‡ºç° {len(items)} æ¬¡\n")
            f.write("-" * 60 + "\n")
            
            for item in items[:5]:  # æ¯ä¸ªè¯æœ€å¤šæ˜¾ç¤º5ä¸ªä¾‹å­
                f.write(f"ID: {item['id']}\n")
                f.write(f"è¯‘æ–‡: {item['zh']}\n")
                f.write("\n")
            
            if len(items) > 5:
                f.write(f"... è¿˜æœ‰ {len(items) - 5} ä¸ªç›¸åŒé—®é¢˜\n")
            
            f.write("\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ£€æµ‹å¹¶ä¿®å¤ç¿»è¯‘ä¸­æ®‹ç•™çš„è‹±æ–‡å•è¯",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åªæ£€æµ‹ï¼Œä¸ä¿®å¤
  python tools/fix_english_leakage.py outputs/llm_results/batch_0001.jsonl --check-only

  # æ£€æµ‹å¹¶è‡ªåŠ¨ä¿®å¤
  python tools/fix_english_leakage.py outputs/llm_results/batch_0001.jsonl --fix --model qwen3:8b

  # æ‰¹é‡å¤„ç†ç›®å½•
  python tools/fix_english_leakage.py outputs/llm_results --fix --model qwen3:8b

  # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
  python tools/fix_english_leakage.py outputs/llm_results/batch_0001.jsonl --check-only --report quality_report.txt
        """
    )
    
    parser.add_argument(
        "input",
        help="è¾“å…¥ JSONL æ–‡ä»¶æˆ–ç›®å½•"
    )
    parser.add_argument(
        "--check-only",
        action="store_true",
        help="åªæ£€æµ‹é—®é¢˜ï¼Œä¸ä¿®å¤"
    )
    parser.add_argument(
        "--fix",
        action="store_true",
        help="è‡ªåŠ¨ä¿®å¤ï¼ˆé‡æ–°ç¿»è¯‘æœ‰é—®é¢˜çš„å¥å­ï¼‰"
    )
    parser.add_argument(
        "--model",
        default="qwen3:8b",
        help="ä¿®å¤æ—¶ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ qwen3:8bï¼‰"
    )
    parser.add_argument(
        "--host",
        default="http://127.0.0.1:11434",
        help="Ollama åœ°å€ï¼ˆé»˜è®¤ http://127.0.0.1:11434ï¼‰"
    )
    parser.add_argument(
        "--report",
        help="ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶"
    )
    parser.add_argument(
        "--output-suffix",
        default="_fixed",
        help="ä¿®å¤åæ–‡ä»¶çš„åç¼€ï¼ˆé»˜è®¤ _fixedï¼‰"
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    
    # æ”¶é›†è¦å¤„ç†çš„æ–‡ä»¶
    files: list[Path] = []
    if input_path.is_dir():
        files = sorted(input_path.glob("*.jsonl"))
        files = [f for f in files if not f.stem.endswith("_fixed")]
    elif input_path.is_file():
        files = [input_path]
    else:
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return 1
    
    if not files:
        print(f"âŒ æœªæ‰¾åˆ° JSONL æ–‡ä»¶")
        return 1
    
    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ” ç¿»è¯‘è´¨é‡æ£€æµ‹" + (" & ä¿®å¤" if args.fix else ""))
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    total_files = len(files)
    total_items = 0
    total_problems = 0
    total_fixed = 0
    
    all_problematic: list[dict] = []
    
    for i, jsonl_file in enumerate(files, 1):
        print(f"[{i}/{total_files}] æ£€æµ‹: {jsonl_file.name}")
        
        # åˆ†ææ–‡ä»¶
        problematic, count, problem_count = analyze_jsonl(jsonl_file)
        total_items += count
        total_problems += problem_count
        all_problematic.extend(problematic)
        
        if problem_count == 0:
            print(f"  âœ“ æ— é—®é¢˜\n")
            continue
        
        print(f"  âš  å‘ç° {problem_count}/{count} æ¡æœ‰é—®é¢˜ ({100*problem_count/count:.1f}%)")
        
        # æ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
        for item in problematic[:3]:
            words = ', '.join(item['leaked_words'])
            print(f"    - æ®‹ç•™è¯: [{words}]")
            print(f"      è¯‘æ–‡: {item['zh'][:60]}...")
        
        if problem_count > 3:
            print(f"    ... è¿˜æœ‰ {problem_count - 3} ä¸ªé—®é¢˜")
        
        # ä¿®å¤æ¨¡å¼
        if args.fix:
            print(f"\n  ğŸ”§ å¼€å§‹ä¿®å¤...")
            
            # è¯»å–åŸå§‹æ•°æ®
            all_items: list[dict] = []
            with jsonl_file.open('r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            all_items.append(json.loads(line))
                        except:
                            pass
            
            # ä¿®å¤æœ‰é—®é¢˜çš„æ¡ç›®
            fixed_count = 0
            problem_ids = {item['id'] for item in problematic}
            
            for item in all_items:
                if item.get('id') in problem_ids:
                    old_zh = item.get('zh', '')
                    print(f"    ä¿®å¤: {item['id']}")
                    print(f"      æ—§: {old_zh[:50]}...")
                    
                    # é‡æ–°ç¿»è¯‘
                    fixed_zh = fix_translation(old_zh, args.host, args.model)
                    
                    if fixed_zh and not detect_english_words(fixed_zh):
                        item['zh'] = fixed_zh
                        fixed_count += 1
                        print(f"      æ–°: {fixed_zh[:50]}...")
                        print(f"      âœ“ ä¿®å¤æˆåŠŸ")
                    else:
                        print(f"      âœ— ä¿®å¤å¤±è´¥ï¼Œä¿æŒåŸæ ·")
            
            # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
            output_file = jsonl_file.parent / f"{jsonl_file.stem}{args.output_suffix}.jsonl"
            with output_file.open('w', encoding='utf-8') as f:
                for item in all_items:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            
            total_fixed += fixed_count
            print(f"\n  âœ“ å·²ä¿®å¤ {fixed_count}/{problem_count} æ¡")
            print(f"  ä¿å­˜åˆ°: {output_file.name}\n")
        else:
            print()
    
    # æ€»ç»“
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    print(f"  æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"  æ€»ç¿»è¯‘æ•°: {total_items}")
    print(f"  é—®é¢˜æ•°: {total_problems} ({100*total_problems/total_items:.2f}%)")
    
    if args.fix:
        print(f"  å·²ä¿®å¤: {total_fixed}")
        print(f"  ä¿®å¤ç‡: {100*total_fixed/total_problems:.1f}%")
    
    print()
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        report_path = Path(args.report)
        generate_report(all_problematic, total_items, report_path)
        print(f"âœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}\n")
    
    return 0


if __name__ == "__main__":
    exit(main())
