#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate_dict.py â€” è‡ªåŠ¨ç”Ÿæˆæ¸¸æˆæœ¯è¯­å­—å…¸

åŠŸèƒ½:
1. ä»å·²ç¿»è¯‘çš„ JSONL ä¸­æå–é«˜é¢‘è¯æ±‡
2. è¯†åˆ«æ¸¸æˆç‰¹å®šæœ¯è¯­ï¼ˆè§’è‰²åã€åœ°ç‚¹åã€é“å…·åç­‰ï¼‰
3. ç”Ÿæˆ CSV æ ¼å¼çš„æœ¯è¯­å­—å…¸
4. æ”¯æŒåˆå¹¶ç°æœ‰å­—å…¸å’Œæ–°ç”Ÿæˆçš„å†…å®¹

ç”¨æ³•ç¤ºä¾‹:
  # ä»æå–çš„æ–‡ä»¶ç”Ÿæˆå­—å…¸
  python tools/generate_dict.py outputs/extract/project_en.jsonl -o outputs/dictionaries

  # æŒ‡å®šæ¸¸æˆåç§°ï¼ˆç”¨äºåˆ†ç»„ï¼‰
  python tools/generate_dict.py outputs/extract/project_en.jsonl -o outputs/dictionaries --game-name "MyGame"

  # ä»å·²ç¿»è¯‘çš„æ–‡ä»¶ç”Ÿæˆï¼ˆæ›´å‡†ç¡®ï¼‰
  python tools/generate_dict.py outputs/prefilled/translated.jsonl -o outputs/dictionaries --from-translated

  # åˆå¹¶ç°æœ‰å­—å…¸
  python tools/generate_dict.py outputs/extract/project_en.jsonl -o outputs/dictionaries --merge data/dictionaries/common_terms.csv
"""

from __future__ import annotations

import argparse
import csv
import json
import re
from collections import Counter, defaultdict
from pathlib import Path


# ========================================
# æœ¯è¯­è¯†åˆ«è§„åˆ™
# ========================================

class TermExtractor:
    """æœ¯è¯­æå–å™¨"""
    
    # å¸¸è§çš„è§’è‰²åç§°æ¨¡å¼
    NAME_PATTERNS = [
        r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?\b',  # John, Mary Smith
        r'\b[A-Z]{2,}\b',  # MC, NPC
    ]
    
    # éœ€è¦è·³è¿‡çš„å¸¸è§è¯
    SKIP_WORDS = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
        'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
        'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this',
        'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
        'my', 'your', 'his', 'her', 'its', 'our', 'their', 'me', 'him', 'us',
        'them', 'what', 'which', 'who', 'when', 'where', 'why', 'how',
        'all', 'each', 'every', 'both', 'few', 'more', 'most', 'other',
        'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',
        'than', 'too', 'very', 'just', 'now', 'then',
        # Ren'Py å¸¸è§å…³é”®è¯
        'scene', 'show', 'hide', 'menu', 'jump', 'call', 'return',
        'elif', 'while', 'pass', 'break', 'continue',
        'True', 'False', 'None',
    }
    
    # æ¸¸æˆç‰¹å®šæœ¯è¯­ç±»å‹
    TERM_TYPES = {
        'character': r'\b(?:mom|dad|sister|brother|son|daughter|wife|husband|girlfriend|boyfriend|friend|teacher|student|boss|neighbor|landlord|landlady|roommate)\b',
        'location': r'\b(?:room|bedroom|bathroom|kitchen|living room|office|school|classroom|gym|pool|beach|park|store|shop|restaurant|bar|club|hospital|hotel)\b',
        'item': r'\b(?:key|phone|wallet|bag|book|note|letter|photo|picture|gift|drink|food|clothes|dress|shirt|pants|shoes|hat|glasses|ring|necklace|bracelet)\b',
        'action': r'\b(?:love|lust|corruption|submission|dominance|affection|relationship|quest|event|scene|route|ending)\b',
        'stat': r'\b(?:level|points|stats|money|cash|gold|energy|stamina|health|mana|experience|exp)\b',
    }
    
    def __init__(self, min_freq: int = 3, min_length: int = 3):
        """
        åˆå§‹åŒ–æœ¯è¯­æå–å™¨
        
        Args:
            min_freq: æœ€å°å‡ºç°é¢‘ç‡
            min_length: æœ€å°è¯é•¿
        """
        self.min_freq = min_freq
        self.min_length = min_length
        self.term_counter: Counter[str] = Counter()
        self.term_types: dict[str, set[str]] = defaultdict(set)
    
    def extract_from_text(self, text: str):
        """ä»æ–‡æœ¬ä¸­æå–æœ¯è¯­"""
        if not text:
            return
        
        # ç§»é™¤ Ren'Py æ ‡ç­¾
        clean = re.sub(r'\{[/a-z_]+\}', '', text, flags=re.IGNORECASE)
        clean = re.sub(r'\[[a-z_]+\]', '', clean, flags=re.IGNORECASE)
        
        # æå–å•è¯
        words = re.findall(r'\b[A-Za-z]+(?:[A-Za-z\-\']*[A-Za-z]+)?\b', clean)
        
        for word in words:
            word_lower = word.lower()
            
            # è·³è¿‡å¸¸è§è¯å’ŒçŸ­è¯
            if word_lower in self.SKIP_WORDS or len(word) < self.min_length:
                continue
            
            # ç»Ÿè®¡è¯é¢‘
            self.term_counter[word] += 1
            
            # è¯†åˆ«æœ¯è¯­ç±»å‹
            for term_type, pattern in self.TERM_TYPES.items():
                if re.search(pattern, word_lower):
                    self.term_types[term_type].add(word)
    
    def extract_names(self, text: str):
        """æå–å¯èƒ½çš„è§’è‰²åç§°"""
        names: set[str] = set()
        
        for pattern in self.NAME_PATTERNS:
            matches = re.findall(pattern, text)
            for match in matches:
                if match.lower() not in self.SKIP_WORDS and len(match) >= 2:
                    names.add(match)
                    self.term_counter[match] += 1
                    self.term_types['character'].add(match)
        
        return names
    
    def get_high_freq_terms(self) -> list[tuple[str, int]]:
        """è·å–é«˜é¢‘æœ¯è¯­"""
        return [
            (term, count)
            for term, count in self.term_counter.most_common()
            if count >= self.min_freq
        ]
    
    def get_terms_by_type(self, term_type: str) -> list[str]:
        """è·å–ç‰¹å®šç±»å‹çš„æœ¯è¯­"""
        return sorted(self.term_types.get(term_type, set()))


# ========================================
# å­—å…¸ç”Ÿæˆ
# ========================================

class DictionaryGenerator:
    """å­—å…¸ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        game_name: str = "game",
        min_freq: int = 3,
        min_length: int = 3
    ):
        """
        åˆå§‹åŒ–å­—å…¸ç”Ÿæˆå™¨
        
        Args:
            game_name: æ¸¸æˆåç§°ï¼ˆç”¨äºåˆ†ç»„ï¼‰
            min_freq: æœ€å°è¯é¢‘
            min_length: æœ€å°è¯é•¿
        """
        self.game_name = game_name
        self.extractor = TermExtractor(min_freq, min_length)
        self.existing_dict: dict[str, dict[str, str]] = {}
    
    def load_existing_dict(self, dict_path: Path):
        """åŠ è½½ç°æœ‰å­—å…¸"""
        if not dict_path.exists():
            return
        
        print(f"  åŠ è½½ç°æœ‰å­—å…¸: {dict_path}")
        
        with dict_path.open('r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                en = row.get('variant_en', '') or row.get('canonical_en', '')
                if en:
                    self.existing_dict[en.lower()] = row
        
        print(f"  å·²åŠ è½½ {len(self.existing_dict)} æ¡æœ¯è¯­")
    
    def process_jsonl(self, jsonl_path: Path, _has_translation: bool = False):
        """
        å¤„ç† JSONL æ–‡ä»¶
        
        Args:
            jsonl_path: JSONL æ–‡ä»¶è·¯å¾„
            _has_translation: æ˜¯å¦åŒ…å«ç¿»è¯‘ï¼ˆzh å­—æ®µï¼‰ï¼Œé¢„ç•™å‚æ•°
        """
        print(f"  åˆ†ææ–‡ä»¶: {jsonl_path}")
        
        count = 0
        with jsonl_path.open('r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                
                try:
                    obj = json.loads(line)
                    text = obj.get('en', '')
                    
                    if text:
                        self.extractor.extract_from_text(text)
                        self.extractor.extract_names(text)
                        count += 1
                
                except (ValueError, json.JSONDecodeError):
                    continue
        
        print(f"  å·²åˆ†æ {count} æ¡æ–‡æœ¬")
    
    def generate_dict_entries(self) -> list[dict[str, str]]:
        """ç”Ÿæˆå­—å…¸æ¡ç›®"""
        entries: list[dict[str, str]] = []
        high_freq = self.extractor.get_high_freq_terms()
        
        print(f"\n  æ‰¾åˆ° {len(high_freq)} ä¸ªé«˜é¢‘æœ¯è¯­")
        
        # æŒ‰ç±»å‹ç»„ç»‡æœ¯è¯­
        for term, count in high_freq:
            term_lower = term.lower()
            
            # è·³è¿‡å·²å­˜åœ¨çš„æœ¯è¯­
            if term_lower in self.existing_dict:
                continue
            
            # åˆ¤æ–­æœ¯è¯­ç±»å‹
            term_type = self._classify_term(term)
            canonical = self._get_canonical_form(term)
            
            entry = {
                'group': f"{self.game_name}_{term_type}",
                'canonical_en': canonical,
                'variant_en': term,
                'zh_final': '',  # ç•™ç©ºï¼Œå¾…äººå·¥å¡«å†™
                'source': 'auto_generated',
                'freq': str(count),
                'type': term_type,
            }
            
            entries.append(entry)
        
        # æŒ‰é¢‘ç‡æ’åº
        entries.sort(key=lambda x: int(x.get('freq', 0)), reverse=True)
        
        return entries
    
    def _classify_term(self, term: str) -> str:
        """åˆ†ç±»æœ¯è¯­"""
        term_lower = term.lower()
        
        for term_type, pattern in self.extractor.TERM_TYPES.items():
            if re.search(pattern, term_lower):
                return term_type
        
        # é»˜è®¤ç±»å‹
        if term[0].isupper():
            return 'character'  # é¦–å­—æ¯å¤§å†™å¯èƒ½æ˜¯äººå
        
        return 'general'
    
    def _get_canonical_form(self, term: str) -> str:
        """è·å–è§„èŒƒå½¢å¼"""
        # ç®€å•å¤„ç†ï¼šä½¿ç”¨å°å†™ä½œä¸ºè§„èŒƒå½¢å¼
        return term.lower()
    
    def save_dict(self, output_path: Path, entries: list[dict[str, str]]):
        """ä¿å­˜å­—å…¸"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # CSV å­—æ®µ
        fieldnames = ['group', 'canonical_en', 'variant_en', 'zh_final', 'source', 'freq', 'type']
        
        with output_path.open('w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(entries)
        
        print(f"\n  âœ“ å­—å…¸å·²ä¿å­˜: {output_path}")
        print(f"  å…± {len(entries)} æ¡æ–°æœ¯è¯­")
    
    def save_summary(self, output_path: Path, entries: list[dict[str, str]]):
        """ä¿å­˜æ‘˜è¦ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ï¼‰"""
        summary_path = output_path.parent / (output_path.stem + '_summary.txt')
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type: dict[str, list[dict[str, str]]] = defaultdict(list)
        for entry in entries:
            by_type[entry['type']].append(entry)
        
        with summary_path.open('w', encoding='utf-8') as f:
            f.write("æ¸¸æˆå­—å…¸ç”Ÿæˆæ‘˜è¦\n")
            f.write(f"æ¸¸æˆ: {self.game_name}\n")
            f.write(f"æ€»è®¡: {len(entries)} æ¡æœ¯è¯­\n")
            f.write(f"\n{'='*60}\n\n")
            
            for term_type in sorted(by_type.keys()):
                terms = by_type[term_type]
                f.write(f"ã€{term_type.upper()}ã€‘ ({len(terms)} æ¡)\n")
                f.write(f"{'-'*60}\n")
                
                for entry in terms[:20]:  # æ¯ç±»æœ€å¤šæ˜¾ç¤º20ä¸ª
                    freq = entry.get('freq', '0')
                    f.write(f"  {entry['variant_en']:20s}  (é¢‘ç‡: {freq:>3s})\n")
                
                if len(terms) > 20:
                    f.write(f"  ... è¿˜æœ‰ {len(terms) - 20} ä¸ªæœ¯è¯­\n")
                
                f.write("\n")
        
        print(f"  æ‘˜è¦å·²ä¿å­˜: {summary_path}")


# ========================================
# ä¸»å‡½æ•°
# ========================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨ç”Ÿæˆæ¸¸æˆæœ¯è¯­å­—å…¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python tools/generate_dict.py outputs/extract/project_en.jsonl -o outputs/dictionaries

  # æŒ‡å®šæ¸¸æˆåç§°
  python tools/generate_dict.py outputs/extract/project_en.jsonl -o outputs/dictionaries --game-name "MyGame"

  # ä»å·²ç¿»è¯‘çš„æ–‡ä»¶ç”Ÿæˆï¼ˆæ›´å‡†ç¡®ï¼‰
  python tools/generate_dict.py outputs/prefilled/translated.jsonl -o outputs/dictionaries --from-translated

  # åˆå¹¶ç°æœ‰å­—å…¸
  python tools/generate_dict.py outputs/extract/project_en.jsonl -o outputs/dictionaries \\
    --merge data/dictionaries/common_terms.csv

æ³¨æ„:
  - ç”Ÿæˆçš„å­—å…¸ä¸­ zh_final å­—æ®µä¸ºç©ºï¼Œéœ€è¦äººå·¥å¡«å†™ç¿»è¯‘
  - è‡ªåŠ¨ç”Ÿæˆçš„æœ¯è¯­æŒ‰é¢‘ç‡æ’åºï¼Œé¢‘ç‡è¶Šé«˜è¶Šé‡è¦
  - å»ºè®®ç»“åˆæ¸¸æˆå†…å®¹å’Œä¸Šä¸‹æ–‡æ‰‹åŠ¨å®¡æ ¸å’Œå®Œå–„å­—å…¸
        """
    )
    
    parser.add_argument(
        "input",
        help="è¾“å…¥ JSONL æ–‡ä»¶ï¼ˆåŒ…å« en å­—æ®µï¼‰"
    )
    parser.add_argument(
        "-o", "--output",
        required=True,
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--game-name",
        default="game",
        help="æ¸¸æˆåç§°ï¼ˆç”¨äºåˆ†ç»„ï¼Œé»˜è®¤ 'game'ï¼‰"
    )
    parser.add_argument(
        "--min-freq",
        type=int,
        default=3,
        help="æœ€å°è¯é¢‘ï¼ˆé»˜è®¤ 3ï¼‰"
    )
    parser.add_argument(
        "--min-length",
        type=int,
        default=3,
        help="æœ€å°è¯é•¿ï¼ˆé»˜è®¤ 3ï¼‰"
    )
    parser.add_argument(
        "--from-translated",
        action="store_true",
        help="è¾“å…¥æ–‡ä»¶åŒ…å«ç¿»è¯‘ï¼ˆzh å­—æ®µï¼‰"
    )
    parser.add_argument(
        "--merge",
        help="åˆå¹¶ç°æœ‰å­—å…¸æ–‡ä»¶ï¼ˆCSV æ ¼å¼ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è·¯å¾„
    input_path = Path(args.input)
    output_dir = Path(args.output)
    
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return 1
    
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ” è‡ªåŠ¨ç”Ÿæˆæ¸¸æˆå­—å…¸")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    print(f"æ¸¸æˆåç§°: {args.game_name}")
    print(f"æœ€å°è¯é¢‘: {args.min_freq}")
    print(f"æœ€å°è¯é•¿: {args.min_length}\n")
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = DictionaryGenerator(
        game_name=args.game_name,
        min_freq=args.min_freq,
        min_length=args.min_length
    )
    
    # åŠ è½½ç°æœ‰å­—å…¸
    if args.merge:
        merge_path = Path(args.merge)
        if merge_path.exists():
            generator.load_existing_dict(merge_path)
        else:
            print(f"  âš  åˆå¹¶å­—å…¸ä¸å­˜åœ¨: {merge_path}")
    
    # å¤„ç†è¾“å…¥æ–‡ä»¶
    generator.process_jsonl(input_path, args.from_translated)
    
    # ç”Ÿæˆå­—å…¸æ¡ç›®
    entries = generator.generate_dict_entries()
    
    if not entries:
        print("\n  âš  æœªæ‰¾åˆ°æ–°çš„é«˜é¢‘æœ¯è¯­")
        print("  å»ºè®®:")
        print("    1. é™ä½ --min-freq å‚æ•°")
        print("    2. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„è‹±æ–‡æ–‡æœ¬")
        return 0
    
    # ä¿å­˜å­—å…¸
    output_file = output_dir / f"{args.game_name}_dict.csv"
    generator.save_dict(output_file, entries)
    
    # ä¿å­˜æ‘˜è¦
    generator.save_summary(output_file, entries)
    
    print(f"\n{'='*60}")
    print("âœ“ å­—å…¸ç”Ÿæˆå®Œæˆ!")
    print(f"{'='*60}\n")
    print("ä¸‹ä¸€æ­¥:")
    print(f"  1. ç¼–è¾‘ {output_file.name}")
    print("  2. å¡«å†™ zh_final åˆ—çš„ä¸­æ–‡ç¿»è¯‘")
    print("  3. å°†å­—å…¸ç”¨äº prefill æ­¥éª¤")
    print()
    
    return 0


if __name__ == "__main__":
    exit(main())
